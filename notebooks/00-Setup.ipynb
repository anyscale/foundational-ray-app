{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundational Ray Application\n",
    "\n",
    "<div align=\"left\">\n",
    "<a target=\"_blank\" href=\"https://console.anyscale.com/\"><img src=\"https://img.shields.io/badge/üöÄ Run_on-Anyscale-9hf\"></a>&nbsp;\n",
    "<a href=\"https://github.com/anyscale/foundational-ray-app\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&amp;message=View%20On%20GitHub&amp;color=586069&amp;logo=github&amp;labelColor=2f363d\"></a>&nbsp;\n",
    "</div>\n",
    "\n",
    "In this guide, we will learn how to:\n",
    "- üí° Create an end-to-end ML application that leverages data processing, batch inference, model training and online serving.\n",
    "- üìà Scale out these workloads in a highly distributed manner -- all in Python.\n",
    "- üíª Develop these workloads with the compute, dependencies, observability, debugger, etc.\n",
    "- üöÄ Optimize all of these workloads with RayTurbo ([data](https://docs.anyscale.com/rayturbo/generated/rayturbo-data), [train](https://docs.anyscale.com/rayturbo/generated/rayturbo-train), [serve](https://docs.anyscale.com/rayturbo/generated/rayturbo-serve)) across performance, fault tolerance, scale and observability.\n",
    "- ‚úÖ Productionize these workloads into batch jobs and online services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "In this tutorial, we'll be implementing an application that leverages the following workloads:\n",
    "\n",
    "- **Data workloads**:\n",
    "    - distributed data ingestion with [Ray Data](https://docs.ray.io/en/latest/data/data.html)\n",
    "    - batch inference to generate embeddings from images\n",
    "    - distributed data processing prior to training\n",
    "- **Train workloads**:  \n",
    "    - distributed model training with [Ray Train](https://docs.ray.io/en/latest/train/train.html) and PyTorch\n",
    "    - save model artifacts to a model registry\n",
    "- **Serve workloads**:\n",
    "    - distributed online serving with [Ray Serve](https://docs.ray.io/en/latest/serve/index.html)\n",
    "    - chain models / processes together and autoscale independently\n",
    "- **Production workloads**:\n",
    "    - create production batch [Jobs](https://docs.anyscale.com/platform/jobs/) for offline workloads (embedding generation, model training, etc.)\n",
    "    - create production online [Services](https://docs.anyscale.com/platform/services/) with our trained models\n",
    "\n",
    "<img src=\"/home/ray/default/images/overview.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workspaces\n",
    "We're developing our application on Anyscale Workspaces, which enables us to develop without thinking about infrastructure, just like we would on a laptop. Workspaces come with:\n",
    "- **Development tools**: build with familiar tools like VS Code, Jupyter notebooks, terminal, observabilty dasboards (including workload specific ones), unified logs, etc.\n",
    "- **Compute**: define the compute our workloads need (or autoscale)\n",
    "    - Head node: manages the cluster, distributes tasks, and hosts development tools.\n",
    "    - Worker nodes: machines that execute work orchestrated by the head node and can scale up and back down to 0.\n",
    "- **Dependency management**: define the environment and it's dependendies your workloads neeed.\n",
    "\n",
    "Learn more about Anyscale Workspaces through the [official documentation](https://docs.anyscale.com/platform/workspaces/).\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> <b> No infrastrucuture headaches</b> \n",
    "\n",
    "It's hard enough for ML/AI developers to develop applications that work in production, they should'nt have to deal with infrastructure pains as well. The ability to define a cluster with heterogenous instances and use them for any workload within seconds is the kind of experience we deserve. Luckily, Anyscale‚Äôs philosophy is *minimal configuration*, *maximal productivity*. \n",
    "\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the infrastructure pains Anyscale removes</summary>\n",
    "\n",
    "üöÄ 1. Fast Workload Launch (No Cluster Setup Required)\n",
    "* With Kubernetes (EKS/GKE), you must manually create a cluster before launching anything.\n",
    "* This includes setting up VPCs, IAM roles, node pools, autoscaling, etc.\n",
    "* Anyscale handles all of this automatically -- you just define your job or endpoint and run it.\n",
    "\n",
    "‚öôÔ∏è 2. No GPU Driver Hassles\n",
    "* Kubernetes requires you to install and manage NVIDIA drivers and the device plugin for GPU workloads.\n",
    "* On Anyscale, GPU environments just work‚Äîdrivers, libraries, and runtime are pre-configured.\n",
    "\n",
    "üì¶ 3. No KubeRay or CRD Management\n",
    "* Running Ray on K8s needs:\n",
    "    * Installing KubeRay\n",
    "    * Writing and maintaining custom YAML manifests\n",
    "    * Managing Custom Resource Definitions (CRDs)\n",
    "    * Tuning stateful sets and pod configs\n",
    "* On Anyscale, this is all abstracted ‚Äî you launch Ray clusters without writing a single YAML file.\n",
    "\n",
    "üß† 4. No Need to Learn K8s Internals\n",
    "* With Kubernetes, users must:\n",
    "    * Inspect pods/logs\n",
    "    * Navigate dashboards\n",
    "    * Manually send HTTP requests to Ray endpoints\n",
    "* Anyscale users never touch pods. Everything is accessible via the CLI, SDK, or UI.\n",
    "\n",
    "üí∏ 5. Spot Instance Handling Just Works\n",
    "* Kubernetes requires custom node pools and lifecycle handling for spot instance preemptions.\n",
    "* With Anyscale, preemptible VMs are handled automatically with node draining and rescheduling.\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend adding the [`autoreload`](https://ipython.org/ipython-doc/3/config/extensions/autoreload.html) cell above as it reloads modules before executing user code. This way the modules you update but previously imported will stay up-to-date automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ray\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyscale Workspaces have a ton more features:\n",
    "- [distributed debugger](https://docs.anyscale.com/platform/workspaces/workspaces-debugging/) across all workers\n",
    "- [unified log viewer](https://docs.anyscale.com/monitoring/accessing-logs) across all workers\n",
    "- [connecting to GitHub](https://docs.anyscale.com/platform/workspaces/workspaces-git)\n",
    "- exposing additional [ports](https://docs.anyscale.com/platform/workspaces/workspaces-ports/)\n",
    "- and more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Note**: We'll be running multiple notebooks in this tutorial so we're going to shutdown each notebook once we're done so that we can free up memory and any copmute resources we might be using.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown this notebook\n",
    "import IPython\n",
    "IPython.get_ipython().kernel.do_shutdown(restart=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
