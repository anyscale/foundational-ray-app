{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed training\n",
    "\n",
    "<div align=\"left\">\n",
    "<a target=\"_blank\" href=\"https://console.anyscale.com/\"><img src=\"https://img.shields.io/badge/üöÄ Run_on-Anyscale-9hf\"></a>&nbsp;\n",
    "<a href=\"https://github.com/anyscale/foundational-ray-app\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&amp;message=View%20On%20GitHub&amp;color=586069&amp;logo=github&amp;labelColor=2f363d\"></a>&nbsp;\n",
    "</div>\n",
    "\n",
    "In this tutorial, we'll execute a distributed training workload that will connect the following heterogenous workloads:\n",
    "- preprocess the dataset prior to training\n",
    "- launch PyTorch distributed training with Ray Train and explore the observability features that come included with Ray Train\n",
    "- evaluation (batch inference + eval logic)\n",
    "- save model artifacts to a model registry (MLOps)\n",
    "\n",
    "**Note**: we won't be tuning our model in this tutorial but be sure to check out [Ray Tune](https://docs.ray.io/en/latest/tune/index.html) for experiment execution and hyperparameter tuning at any scale.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/distributed_training.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ray\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Ray Train v2 (it's too good to wait for public release!)\n",
    "ray.init(\n",
    "    runtime_env={\n",
    "        \"env_vars\": {\"RAY_TRAIN_V2_ENABLED\": \"1\"}, \n",
    "        \"working_dir\": \"/home/ray/default\",  # to import doggos (default working_dir=\".\")\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# This will be removed once Ray Train v2 is part of latest Ray version\n",
    "echo \"RAY_TRAIN_V2_ENABLED=1\" > /home/ray/default/.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load env vars in notebooks\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert our classes to labels (unique integers) so that we can train a classifier that can correctly predict the class given an input image. But before we do this, we'll quickly apply the same data ingestion and preprocessing as the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_class(row):\n",
    "    row[\"class\"] = row[\"path\"].rsplit(\"/\", 3)[-2]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data splits\n",
    "train_ds = ray.data.read_images(\"s3://doggos-dataset/train\", include_paths=True, shuffle=\"files\")\n",
    "train_ds = train_ds.map(add_class)\n",
    "val_ds = ray.data.read_images(\"s3://doggos-dataset/val\", include_paths=True)\n",
    "val_ds = val_ds.map(add_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a `Preprocessor` class that will:\n",
    "- create an embedding, since we will not want to change the embedding layer's weights and so we don't have to do it repeatedly as part of the model's forward pass (unecessary compute)\n",
    "- convert our classes into labels for the classifier. \n",
    "\n",
    "While we could've just done this as a simple operation, we're taking the time to organize it as a class so that we can save and load for inference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from doggos.embed import EmbeddingGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \"\"\"Preprocessor class.\"\"\"\n",
    "    def __init__(self, class_to_label=None):\n",
    "        self.class_to_label = class_to_label or {}  # mutable defaults\n",
    "        self.label_to_class = {v: k for k, v in self.class_to_label.items()}\n",
    "        \n",
    "    def fit(self, ds, column):\n",
    "        self.classes = ds.unique(column=column)\n",
    "        self.class_to_label = {tag: i for i, tag in enumerate(self.classes)}\n",
    "        self.label_to_class = {v: k for k, v in self.class_to_label.items()}\n",
    "        return self\n",
    "\n",
    "    def convert_to_label(self, row, class_to_label):\n",
    "        if \"class\" in row:\n",
    "            row[\"label\"] = class_to_label[row[\"class\"]]\n",
    "        return row\n",
    "    \n",
    "    def transform(self, ds, concurrency=4, batch_size=64, num_gpus=1):\n",
    "        ds = ds.map(\n",
    "            self.convert_to_label, \n",
    "            fn_kwargs={\"class_to_label\": self.class_to_label},\n",
    "        )\n",
    "        ds = ds.map_batches(\n",
    "            EmbeddingGenerator,\n",
    "            fn_constructor_kwargs={\"model_id\": \"openai/clip-vit-base-patch32\"},\n",
    "            fn_kwargs={\"device\": \"cuda\"}, \n",
    "            concurrency=concurrency, \n",
    "            batch_size=batch_size,\n",
    "            num_gpus=num_gpus,\n",
    "        )\n",
    "        ds = ds.drop_columns([\"image\"])\n",
    "        return ds\n",
    "\n",
    "    def save(self, fp):\n",
    "        with open(fp, \"w\") as f:\n",
    "            json.dump(self.class_to_label, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "preprocessor = Preprocessor()\n",
    "preprocessor = preprocessor.fit(train_ds, column=\"class\")\n",
    "train_ds = preprocessor.transform(ds=train_ds)\n",
    "val_ds = preprocessor.transform(ds=val_ds)\n",
    "train_ds.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b> Data Processing</b> \n",
    "\n",
    "Be sure to checkout this extensive guide on [data loading and preprocessing](https://docs.ray.io/en/latest/train/user-guides/data-loading-preprocessing.html) for the last-mile preprocessing we'll need to do prior to training our models. However, Ray Data does support performant joins, filters, aggregations, etc. for the more structure data processing your workloads may need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b> Store often, Save compute</b> \n",
    "\n",
    "We're going to now store our preprocessed data into shared cloud storage because we want to:\n",
    "- save a record of what this preprocessed data looks like\n",
    "- avoid triggering the entire preprocessing for each batch our model will process\n",
    "- don't want to [`materialize`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.materialize.html) the preprocessed data either (shouldn't force large data to fit in memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from doggos.utils import delete_s3_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write processed data to cloud storage\n",
    "preprocessed_data_path = os.path.join(\n",
    "    os.getenv(\"ANYSCALE_ARTIFACT_STORAGE\", \"\"), \n",
    "    os.getenv(\"ANYSCALE_USERNAME\", \"\").replace(\" \", \"_\"), \n",
    "    \"doggos/preprocessed_data\",\n",
    ")\n",
    "delete_s3_objects(s3_path=preprocessed_data_path)\n",
    "preprocessed_train_path = os.path.join(preprocessed_data_path, \"preprocessed_train\")\n",
    "preprocessed_val_path = os.path.join(preprocessed_data_path, \"preprocessed_val\")\n",
    "train_ds.write_parquet(preprocessed_train_path)\n",
    "val_ds.write_parquet(preprocessed_val_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our model -- a simple two layer neural net with softmax layer to predict class probabilities. You'll notice that it's all just base PyTorch and nothing else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, dropout_p, num_classes):\n",
    "        super().__init__()\n",
    "        # Hyperparameters\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout_p = dropout_p\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        z = self.fc1(batch[\"embedding\"])\n",
    "        z = self.batch_norm(z)\n",
    "        z = self.relu(z)\n",
    "        z = self.dropout(z)\n",
    "        z = self.fc2(z)\n",
    "        return z\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def predict(self, batch):\n",
    "        z = self(batch)\n",
    "        y_pred = torch.argmax(z, dim=1).cpu().numpy()\n",
    "        return y_pred\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def predict_probabilities(self, batch):\n",
    "        z = self(batch)\n",
    "        y_probs = F.softmax(z, dim=1).cpu().numpy()\n",
    "        return y_probs\n",
    "\n",
    "    def save(self, dp):\n",
    "        Path(dp).mkdir(parents=True, exist_ok=True)\n",
    "        with open(Path(dp, \"args.json\"), \"w\") as fp:\n",
    "            json.dump({\n",
    "                \"embedding_dim\": self.embedding_dim,\n",
    "                \"hidden_dim\": self.hidden_dim,\n",
    "                \"dropout_p\": self.dropout_p,\n",
    "                \"num_classes\": self.num_classes,\n",
    "            }, fp, indent=4)\n",
    "        torch.save(self.state_dict(), Path(dp, \"model.pt\"))\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, args_fp, state_dict_fp, device=\"cpu\"):\n",
    "        with open(args_fp, \"r\") as fp:\n",
    "            model = cls(**json.load(fp))\n",
    "        model.load_state_dict(torch.load(state_dict_fp, map_location=device))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "num_classes = len(preprocessor.classes)\n",
    "model = ClassificationModel(\n",
    "    embedding_dim=512, \n",
    "    hidden_dim=256, \n",
    "    dropout_p=0.3, \n",
    "    num_classes=num_classes,\n",
    ")\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a sample batch of data and ensure that tensors of the proper data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.torch import get_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    dtypes = {\"embedding\": torch.float32, \"label\": torch.int64}\n",
    "    tensor_batch = {}\n",
    "    for key in dtypes.keys():\n",
    "        if key in batch:\n",
    "            tensor_batch[key] = torch.as_tensor(\n",
    "                batch[key],\n",
    "                dtype=dtypes[key],\n",
    "                device=get_device(),\n",
    "            )\n",
    "    return tensor_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample batch\n",
    "sample_batch = train_ds.take_batch(batch_size=3)\n",
    "collate_fn(batch=sample_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be creating a model registry in our [Anyscale user storage](https://docs.anyscale.com/configuration/storage/#user-storage) to save our model checkpoints to. We'll be using OSS mlflow but we can easily [set up other experiment trackers](https://docs.ray.io/en/latest/train/user-guides/experiment-tracking.html) with Ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_registry = \"/mnt/user_storage/mlflow/doggos\"\n",
    "os.path.isdir(model_registry) and shutil.rmtree(model_registry)  # clean up\n",
    "os.makedirs(model_registry, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define our training workload by specifying our:\n",
    "- experiment and model parameters\n",
    "- compute scaling configuration\n",
    "- forward pass for batches of training and validation data\n",
    "- train loop for each epoch of data (and checkpointing)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/trainer.png\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loop config\n",
    "experiment_name = \"doggos\"\n",
    "train_loop_config = {\n",
    "    \"model_registry\": model_registry,\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"embedding_dim\": 512,\n",
    "    \"hidden_dim\": 256,\n",
    "    \"dropout_p\": 0.3,\n",
    "    \"lr\": 1e-3,\n",
    "    \"lr_factor\": 0.8,\n",
    "    \"lr_patience\": 3,\n",
    "    \"num_epochs\": 20,\n",
    "    \"batch_size\": 256,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling config\n",
    "num_workers = 4\n",
    "scaling_config = ray.train.ScalingConfig(\n",
    "    num_workers=num_workers,\n",
    "    use_gpu=True,\n",
    "    resources_per_worker={\"CPU\": 8, \"GPU\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from ray.train.torch import TorchTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(ds, batch_size, model, num_classes, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    loss = 0.0\n",
    "    ds_generator = ds.iter_torch_batches(batch_size=batch_size, collate_fn=collate_fn)\n",
    "    for i, batch in enumerate(ds_generator):\n",
    "        optimizer.zero_grad()  # reset gradients\n",
    "        z = model(batch)  # forward pass\n",
    "        targets = F.one_hot(batch[\"label\"], num_classes=num_classes).float()\n",
    "        J = loss_fn(z, targets)  # define loss\n",
    "        J.backward()  # backward pass\n",
    "        optimizer.step()  # update weights\n",
    "        loss += (J.detach().item() - loss) / (i + 1)  # cumulative loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(ds, batch_size, model, num_classes, loss_fn):\n",
    "    model.eval()\n",
    "    loss = 0.0\n",
    "    y_trues, y_preds = [], []\n",
    "    ds_generator = ds.iter_torch_batches(batch_size=batch_size, collate_fn=collate_fn)\n",
    "    with torch.inference_mode():\n",
    "        for i, batch in enumerate(ds_generator):\n",
    "            z = model(batch)\n",
    "            targets = F.one_hot(batch[\"label\"], num_classes=num_classes).float()  # one-hot (for loss_fn)\n",
    "            J = loss_fn(z, targets).item()\n",
    "            loss += (J - loss) / (i + 1)\n",
    "            y_trues.extend(batch[\"label\"].cpu().numpy())\n",
    "            y_preds.extend(torch.argmax(z, dim=1).cpu().numpy())\n",
    "    return loss, np.vstack(y_trues), np.vstack(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_per_worker(config):\n",
    "    # Hyperparameters\n",
    "    model_registry = config[\"model_registry\"]\n",
    "    experiment_name = config[\"experiment_name\"]\n",
    "    embedding_dim = config[\"embedding_dim\"]\n",
    "    hidden_dim = config[\"hidden_dim\"]\n",
    "    dropout_p = config[\"dropout_p\"]\n",
    "    lr = config[\"lr\"]\n",
    "    lr_factor = config[\"lr_factor\"]\n",
    "    lr_patience = config[\"lr_patience\"]\n",
    "    num_epochs = config[\"num_epochs\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    num_classes = config[\"num_classes\"]\n",
    "\n",
    "    # Experiment tracking\n",
    "    if ray.train.get_context().get_world_rank() == 0:\n",
    "        mlflow.set_tracking_uri(f\"file:{model_registry}\")\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "        mlflow.start_run()\n",
    "        mlflow.log_params(config)\n",
    "\n",
    "    # Datasets\n",
    "    train_ds = ray.train.get_dataset_shard(\"train\")\n",
    "    val_ds = ray.train.get_dataset_shard(\"val\")\n",
    "\n",
    "    # Model\n",
    "    model = ClassificationModel(\n",
    "        embedding_dim=embedding_dim, \n",
    "        hidden_dim=hidden_dim, \n",
    "        dropout_p=dropout_p, \n",
    "        num_classes=num_classes,\n",
    "    )\n",
    "    model = ray.train.torch.prepare_model(model)\n",
    "\n",
    "    # Training components\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode=\"min\", \n",
    "        factor=lr_factor, \n",
    "        patience=lr_patience,\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    best_val_loss = float(\"inf\")\n",
    "    for epoch in range(num_epochs):\n",
    "        # Steps\n",
    "        train_loss = train_epoch(train_ds, batch_size, model, num_classes, loss_fn, optimizer)\n",
    "        val_loss, _, _ = eval_epoch(val_ds, batch_size, model, num_classes, loss_fn)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Checkpoint (metrics, preprocessor and model artifacts)\n",
    "        with tempfile.TemporaryDirectory() as dp:\n",
    "            model.module.save(dp=dp)\n",
    "            metrics = dict(lr=optimizer.param_groups[0][\"lr\"], train_loss=train_loss, val_loss=val_loss)\n",
    "            with open(os.path.join(dp, \"class_to_label.json\"), \"w\") as fp:\n",
    "                json.dump(config[\"class_to_label\"], fp, indent=4)\n",
    "            if ray.train.get_context().get_world_rank() == 0:  # only on main worker 0\n",
    "                mlflow.log_metrics(metrics, step=epoch)\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    mlflow.log_artifacts(dp)\n",
    "\n",
    "    # End experiment tracking\n",
    "    if ray.train.get_context().get_world_rank() == 0:\n",
    "        mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b> Minimal change to your training code</b> \n",
    "\n",
    "You'll notice that there isn't much new Ray Train code on top of our base PyTorch code. We specified how we want to scale out our training workload, load the Ray datasets and then checkpoint on our main worker node... and that's it! Check out these guides ([PyTorch](https://docs.ray.io/en/latest/train/getting-started-pytorch.html), [PyTorch Lightning](https://docs.ray.io/en/latest/train/getting-started-pytorch-lightning.html), [HuggingFace Transformers](https://docs.ray.io/en/latest/train/getting-started-transformers.html)) to see the minimal delta code needed to distribute our training workloads and check out this extensive list of [Ray Train user guides](https://docs.ray.io/en/latest/train/user-guides.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed datasets\n",
    "preprocessed_train_ds = ray.data.read_parquet(preprocessed_train_path)\n",
    "preprocessed_val_ds = ray.data.read_parquet(preprocessed_val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "train_loop_config[\"class_to_label\"] = preprocessor.class_to_label\n",
    "train_loop_config[\"num_classes\"] = len(preprocessor.class_to_label)\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    train_loop_config=train_loop_config,\n",
    "    scaling_config=scaling_config,\n",
    "    datasets={\"train\": preprocessed_train_ds, \"val\": preprocessed_val_ds},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b> Ray Train</b> \n",
    "\n",
    "**üéõÔ∏è Multi-node orchestration made easy**\n",
    "\n",
    "- Ray Train automatically handles multi-node, multi-GPU setup with no manual SSH setup or hostfile configs. \n",
    "- And it also integrates with Ray's cluster launcher for cloud (AWS, GCP, K8s) and on-prem clusters. \n",
    "- Solutions like PyTorch DDP require manually setting up your own process group, ranks, networking, etc.\n",
    "\n",
    "**ü©π 2. Built-in fault tolerance**\n",
    "- Ray Train supports automatic retry of failed workers.\n",
    "- and can continue training from the last checkpoint in case of failure.\n",
    "\n",
    "\n",
    "**‚úÇÔ∏è 3. Flexible training strategies** (not just DDP)\n",
    "- Ray Train supports Data Parallel, Model Parallel, Parameter Server, and even custom strategies.\n",
    "- You can also use Torch DDP, FSPD, DeepSpeed, etc. under the hood if you want.\n",
    "- [Ray Compiled graphs](https://docs.ray.io/en/latest/ray-core/compiled-graph/ray-compiled-graph.html) allow us to even define different parallelism for jointly optimizing multipe models (Megatron, Deepspeed, etc. only allow for one global setting).\n",
    "\n",
    "**üî• Better support for heterogeneous clusters**\n",
    "- Ray Train lets you define per-worker resource requirements (e.g., 2 CPUs and 1 GPU per worker).\n",
    "- and can run on heterogeneous machines and scale flexibly (e.g., CPU for preprocessing and GPU for training)\n",
    "\n",
    "**üåç Integrations**\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/train_integrations.png\" width=500>\n",
    "\n",
    "[RayTurbo Train](https://docs.anyscale.com/rayturbo/rayturbo-train) offers even more improvement to the price-performance ratio, performance monitoring and more:\n",
    "- **elastic training** to ccale to a dynamic number of workers, continue training on fewer resources (even on spot instances).\n",
    "- **purpose-built dashboard** designed to streamline the debugging of Ray Train workloads\n",
    "    - Monitoring: View the status of training runs and train workers.\n",
    "    - Metrics: See insights on training throughput, training system operation time.\n",
    "    - Profiling: Investigate bottlenecks, hangs, or errors from individual training worker processes.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/train_dashboard.png\" width=700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "results = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view our experiment metrics and model artifacts in our model registry. We're using OSS mlflow so we can run the server by pointing to our model registry location:\n",
    "\n",
    "```bash\n",
    "mlflow server -h 0.0.0.0 -p 8080 --backend-store-uri /mnt/user_storage/mlflow/doggos\n",
    "```\n",
    "\n",
    "We can view the dashboard by going to the **Overview tab** up top ‚Üí **Open Ports**. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/mlflow.png\" width=685>\n",
    "\n",
    "We also have our Ray Dashboard and Train workfload specific dashboards above. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/train_metrics.png\" width=700>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted runs\n",
    "mlflow.set_tracking_uri(f\"file:{model_registry}\")\n",
    "sorted_runs = mlflow.search_runs(\n",
    "    experiment_names=[experiment_name], \n",
    "    order_by=[\"metrics.val_loss ASC\"])\n",
    "best_run = sorted_runs.iloc[0]\n",
    "best_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can easily wrap our training workload as a production grade [Anyscale Job](https://docs.anyscale.com/platform/jobs/) ([API ref](https://docs.anyscale.com/reference/job-api/))\n",
    "\n",
    "**Note**: \n",
    "- we're using a `containerfile` to define our dependencies, but we could easily use a pre-built image as well.\n",
    "- we can specify the compute as a [compute config](https://docs.anyscale.com/configuration/compute-configuration/) or inline in a [job config](https://docs.anyscale.com/reference/job-api#job-cli) file.\n",
    "- when we don't specify compute and when launching from a workspace, this defaults to the compute configuration of the Workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Production batch job\n",
    "anyscale job submit --name=train-doggos-model \\\n",
    "  --containerfile=\"/home/ray/default/containerfile\" \\\n",
    "  --working-dir=\"/home/ray/default\" \\\n",
    "  --exclude=\"\" \\\n",
    "  --max-retries=0 \\\n",
    "  -- python doggos/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/train_job.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll conclude by evaluating our trained model on our test dataset. Evaluation is essentially just the same as our batch inference workload -- where we'll apply the model on batches of data and then calculate metrics using the predictions vs.true labels. Ray data is hyper optimized for throughput so preserving order is not a priority. But for evaluation, this is crucial! So we'll achieve this by preserving the entire row and adding the predicted label as another column to each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "from sklearn.metrics import multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchPredictor:\n",
    "    def __init__(self, preprocessor, model):\n",
    "        self.preprocessor = preprocessor\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "\n",
    "    def __call__(self, batch, device=\"cuda\"):\n",
    "        self.model.to(device)\n",
    "        batch[\"prediction\"] = self.model.predict(collate_fn(batch))\n",
    "        return batch\n",
    "\n",
    "    def predict_probabilities(self, batch, device=\"cuda\"):\n",
    "        self.model.to(device)\n",
    "        predicted_probabilities = self.model.predict_probabilities(collate_fn(batch))\n",
    "        batch[\"probabilities\"] = [\n",
    "            {self.preprocessor.label_to_class[i]: prob for i, prob in enumerate(probabilities)}\n",
    "            for probabilities in predicted_probabilities\n",
    "        ]\n",
    "        return batch\n",
    "    \n",
    "    @classmethod\n",
    "    def from_artifacts_dir(cls, artifacts_dir):\n",
    "        with open(os.path.join(artifacts_dir, \"class_to_label.json\"), \"r\") as fp:\n",
    "            class_to_label = json.load(fp)\n",
    "        preprocessor = Preprocessor(class_to_label=class_to_label)\n",
    "        model = ClassificationModel.load(\n",
    "            args_fp=os.path.join(artifacts_dir, \"args.json\"), \n",
    "            state_dict_fp=os.path.join(artifacts_dir, \"model.pt\"),\n",
    "        )\n",
    "        return cls(preprocessor=preprocessor, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preproces eval dataset\n",
    "artifacts_dir = urlparse(best_run.artifact_uri).path\n",
    "predictor = TorchPredictor.from_artifacts_dir(artifacts_dir=artifacts_dir)\n",
    "test_ds = ray.data.read_images(\"s3://doggos-dataset/test\", include_paths=True)\n",
    "test_ds = test_ds.map(add_class)\n",
    "test_ds = predictor.preprocessor.transform(ds=test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred (batch inference)\n",
    "pred_ds = test_ds.map_batches(\n",
    "    predictor,\n",
    "    fn_kwargs={\"device\": \"cuda\"},\n",
    "    concurrency=4,\n",
    "    batch_size=64,\n",
    "    num_gpus=1,\n",
    ")\n",
    "pred_ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_metric(batch):\n",
    "    labels = batch[\"label\"]\n",
    "    preds = batch[\"prediction\"]\n",
    "    mcm = multilabel_confusion_matrix(labels, preds)\n",
    "    tn, fp, fn, tp = [], [], [], []\n",
    "    for i in range(mcm.shape[0]):\n",
    "        tn.append(mcm[i, 0, 0])  # True negatives\n",
    "        fp.append(mcm[i, 0, 1])  # False positives\n",
    "        fn.append(mcm[i, 1, 0])  # False negatives\n",
    "        tp.append(mcm[i, 1, 1])  # True positives\n",
    "    return {\"TN\": tn, \"FP\": fp, \"FN\": fn, \"TP\": tp}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregated metrics after processing all batches\n",
    "metrics_ds = pred_ds.map_batches(batch_metric)\n",
    "aggregate_metrics = metrics_ds.sum([\"TN\", \"FP\", \"FN\", \"TP\"])\n",
    "\n",
    "# Aggregate the confusion matrix components across all batches\n",
    "tn = aggregate_metrics[\"sum(TN)\"]\n",
    "fp = aggregate_metrics[\"sum(FP)\"]\n",
    "fn = aggregate_metrics[\"sum(FN)\"]\n",
    "tp = aggregate_metrics[\"sum(TP)\"]\n",
    "\n",
    "# Calculate metrics\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1: {f1:.2f}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.get_ipython().kernel.do_shutdown(restart=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
